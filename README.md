# Projeto Final â€“ triggo.ai | DataSUS + Snowflake + dbt

## Contexto e Objetivo
Este projeto visa construir uma pipeline completa para ingestÃ£o, transformaÃ§Ã£o e modelagem dos dados pÃºblicos do DataSUS (Sistema Ãšnico de SaÃºde), com foco na otimizaÃ§Ã£o da anÃ¡lise em saÃºde pÃºblica.  
A soluÃ§Ã£o utiliza Snowflake como data warehouse e dbt para a camada de transformaÃ§Ã£o, organizando dados para anÃ¡lise e geraÃ§Ã£o de insights estratÃ©gicos.

## Dados Utilizados
- Fonte: DataSUS â€“ AutorizaÃ§Ãµes de InternaÃ§Ã£o Hospitalar (AIH)  
- Arquivos originais: Formato `.dbc` (compactado)  
- PerÃ­odo: Ãšltimos anos disponÃ­veis no portal oficial  
- Link oficial dos dados: [ftp.datasus.gov.br](ftp://ftp.datasus.gov.br/dissemin/publicos/SIHSUS/200801_/Dados/)

## ğŸ“ Estrutura do RepositÃ³rio
```
ProjetoFinal_triggo/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ converted/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â””â”€â”€ 1_convert_dbc_to_csv.py
â”‚   â”œâ”€â”€ load/
â”‚   â””â”€â”€ transform/
â”‚
â”œâ”€â”€ sql/
|   â”œâ”€â”€ 01_create_warehouse_db_schema.sql
â”‚   â””â”€â”€ 02_create_and_populate_raw_table.sql
â”‚
â”œâ”€â”€ dbt/
â”‚
â”œâ”€â”€ docs/
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Passo a Passo Realizado

### 1. IngestÃ£o e PreparaÃ§Ã£o dos Dados
- Criado warehouse, database e schema dedicados no Snowflake para o projeto, garantindo isolamento e organizaÃ§Ã£o dos dados.
- Convertidos arquivos originais no formato `.dbc` para `.csv` via script Python (`scripts/extract/convert_dbc_to_csv.py`), visando compatibilidade com o Snowflake e o dbt.
- Armazenamento temporÃ¡rio realizado em um stage interno no Snowflake, simulando um data lake inicial.
- Criada tabela `raw_aih` com schema tipado (DATE, NUMBER, STRING) para preservar integridade e coerÃªncia dos dados brutos.
- IngestÃ£o realizada com o comando `COPY INTO`, garantindo carregamento em lote e controle de erros.

DecisÃ£o tÃ©cnica chave:
Manter as datas em formato texto (`YYYYMMDD`) na ingestÃ£o para evitar problemas de parsing inicial. ConversÃ£o para tipo DATE serÃ¡ feita nas camadas de transformaÃ§Ã£o no dbt, assegurando rastreabilidade.

## 2. TransformaÃ§Ã£o e Modelagem com dbt
- Estrutura de camadas adotada no dbt:
  - Staging (`stg_`): limpeza, padronizaÃ§Ã£o de nomes de colunas, conversÃ£o de formatos (ex.: datas), e mapeamento direto da `raw_aih`.
  - Intermediate (`int_`): junÃ§Ã£o e enriquecimento de dados, preparando chaves para dimensÃµes.
  - Mart (`dim_` e `fct_`): modelagem dimensional (Star Schema) com:
    - `dim_tempo`
    - `dim_localidade`
    - `dim_doenca`
    - `dim_procedimento`
    - `fct_internacoes` (tabela fato principal)

- MaterializaÃ§Ãµes usadas:
  - Views para staging, visando leveza e flexibilidade.
  - Tables para dimensÃµes e fato, visando performance em consultas analÃ­ticas.
- Implementados testes de qualidade de dados no dbt:
  - "not_null" e "unique" em IDs e chaves primÃ¡rias.

3. ExplicaÃ§Ã£o da escolha do Design
- Snowflake foi escolhido como data warehouse pela performance, escalabilidade e facilidade de integraÃ§Ã£o com dbt.
- ConversÃ£o para CSV simplifica ingestÃ£o e padroniza entrada, evitando dependÃªncia de formatos proprietÃ¡rios.
- Camadas no dbt permitem modularidade, facilitam manutenÃ§Ã£o e isolam lÃ³gicas de transformaÃ§Ã£o.
- Testes dbt asseguram confiabilidade e documentam regras de negÃ³cio diretamente no cÃ³digo.
- Modelagem dimensional otimiza consumo em ferramentas de BI e permite anÃ¡lises rÃ¡pidas sobre internaÃ§Ãµes hospitalares.

